/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file "/home/ubuntu/.config/matplotlib/matplotlibrc", line #2
  (fname, cnt))
/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file "/home/ubuntu/.config/matplotlib/matplotlibrc", line #3
  (fname, cnt))
/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.
  'Matplotlib is building the font cache using fc-list. '
(2000, 999, 2)
EPOCH:  0
Traceback (most recent call last):
  File "ball_model.py", line 114, in <module>
    optimizer.step(closure)
  File "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lbfgs.py", line 103, in step
    orig_loss = closure()
  File "ball_model.py", line 111, in closure
    print('loss:', loss.data.numpy()[0])
IndexError: too many indices for array
/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file "/home/ubuntu/.config/matplotlib/matplotlibrc", line #2
  (fname, cnt))
/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file "/home/ubuntu/.config/matplotlib/matplotlibrc", line #3
  (fname, cnt))
(300, 999, 2)
EPOCH:  0
loss: 19.774064881181616
loss: 19.58277928447879
loss: 5.0591955635436445
loss: 7.130829350236327
loss: 4.183129432777205
loss: 3.888537250632766
loss: 3.4928017428747573
loss: 3.1937984302013325
loss: 2.880798411213558
loss: 2.4774002582603076
loss: 1.729554545931113
loss: 1.5698906789268539
loss: 1.445883724946487
loss: 1.3557140099162546
loss: 1.3907357615039688
loss: 1.2565766059890497
loss: 1.2273533823591607
loss: 1.168600415802754
loss: 1.1108640106808614
loss: 1.0351353356713222
test loss: 0.9308876150487992
Save model!

EPOCH:  1
loss: 0.9898022054030713
loss: 0.9709311119636008
loss: 0.9531522015970586
loss: 0.88211915640494
loss: 0.8056450468219397
loss: 0.7822096410403799
loss: 0.7150542422428762
loss: 0.6895784301160456
loss: 0.592477101827899
loss: 0.5265673640146618
loss: 0.45647439641139365
loss: 0.3732028746597446
loss: 0.3137222756331102
loss: 0.25466176891426734
loss: 0.2291680076374587
loss: 0.18730944809255742
loss: 0.1641195198691185
loss: 0.1299364890514507
loss: 0.2228520535622915
loss: 0.11368497642914882
test loss: 0.09589385499916799
Save model!

EPOCH:  2
loss: 0.09850129169330035
loss: 0.09106197520366612
loss: 0.08020287910750912
loss: 0.06681653813261458
loss: 0.05789439360836578
loss: 0.05511826227914299
loss: 0.04820460551915061
loss: 0.04232080676443637
loss: 0.03726897940031773
loss: 0.03310929305389954
loss: 0.029457741550926213
loss: 0.028140692198484193
loss: 0.02659452959002575
loss: 0.02518507629334023
loss: 0.023694456233978426
loss: 0.021338298471329408
loss: 0.01855972853029309
loss: 0.01670761175089921
loss: 0.015738680261824067
loss: 0.015109891375658347
test loss: 0.012671341959542962
Save model!

EPOCH:  3
loss: 0.014636571561137188
loss: 0.014193599227197378
loss: 0.013674107890897362
loss: 0.012832690001329788
loss: 0.011199601386625289
loss: 0.009771474900206101
loss: 0.0093391896738862
loss: 0.008505335704213147
loss: 0.0077198216071382485
loss: 0.006823514660082013
loss: 0.006440574663275232
loss: 0.006261726545271749
loss: 0.0059563179561427666
loss: 0.00570925597730269
loss: 0.0053107761562357905
loss: 0.00492762953051407
loss: 0.004255532051444161
loss: 0.003940130831530102
loss: 0.0037983490992960743
loss: 0.003747733845573288
test loss: 0.003498198041817098
Save model!

EPOCH:  4
loss: 0.0036790305577042363
loss: 0.003542041635566717
loss: 0.0033179109412324686
loss: 0.003144867859630312
loss: 0.003018588079163561
loss: 0.002943440102604853
loss: 0.002884873292395472
loss: 0.002846538507436349
loss: 0.0028069121994642437
loss: 0.0027425022237757084
loss: 0.002669713991929037
loss: 0.00258078360427959
loss: 0.002496779774667587
loss: 0.002405673736306742
loss: 0.002327090136743281
loss: 0.0022867857262004214
loss: 0.0022568650673853155
loss: 0.0022174891424799497
loss: 0.0021496705255453167
loss: 0.002081083969965876
test loss: 0.0020250076211072196
Save model!

EPOCH:  5
loss: 0.002042875671750724
loss: 0.0020148129155990796
loss: 0.0019814814884134726
loss: 0.0019325865854889603
loss: 0.0018844058192404906
loss: 0.001840988495554923
loss: 0.001812739288597086
loss: 0.001776570179414769
loss: 0.0017109027366552395
loss: 0.0016477326939867657
loss: 0.0015891258980972607
loss: 0.0015515001561864487
loss: 0.001534853196039578
loss: 0.0015246094782081611
loss: 0.0015142561628319739
loss: 0.0014951531069221941
loss: 0.0014696001496368623
loss: 0.0014292853823624418
loss: 0.001388547696711671
loss: 0.0013541227141338579
test loss: 0.0012933662087534956
Save model!

EPOCH:  6
loss: 0.001316080110473909
loss: 0.0012870178118874847
loss: 0.0012635956481901306
loss: 0.0012453858150807053
loss: 0.0012278627924294154
loss: 0.0012105089991011656
loss: 0.0011936732117730216
loss: 0.0011716667365614802
loss: 0.001152521925721315
loss: 0.0011341130120677667
loss: 0.0011182342165515283
loss: 0.0011075520305071985
loss: 0.0010963292619319686
loss: 0.0010878041571445122
loss: 0.0010728545210849359
loss: 0.0010594415859387254
loss: 0.0010429245072825475
loss: 0.001023302503609216
loss: 0.0010001668831498792
loss: 0.0009722432374678768
test loss: 0.0009214492744133169
Save model!

EPOCH:  7
loss: 0.0009378807191679431
loss: 0.0009076728838512683
loss: 0.0008859925654994581
loss: 0.0008618747333757601
loss: 0.0008278955735057224
loss: 0.0008037160031700028
loss: 0.0007841054193303365
loss: 0.0007664394520182241
loss: 0.0007437187793744762
loss: 0.0007190783652620418
loss: 0.0006945188565341708
loss: 0.0006578455321188519
loss: 0.0006235909685707089
loss: 0.0005910784737698097
loss: 0.000561091376574468
loss: 0.0005275873875832883
loss: 0.000528954416492968
loss: 0.0004939376052844354
loss: 0.000488563212252642
loss: 0.0004851354688826067
test loss: 0.0004498519141079853
Save model!

EPOCH:  8
loss: 0.00048146706991515555
loss: 0.00047422278711342517
loss: 0.00046162899599153215
loss: 0.0004453831923355243
loss: 0.0004189836381648676
loss: 0.000386293175443793
loss: 0.00036616200538124694
loss: 0.000350636263780841
loss: 0.0003435479659574247
loss: 0.000341104094791002
loss: 0.000335915501768769
loss: 0.0003329834677254135
loss: 0.00032678500184600144
loss: 0.0003206720589348509
loss: 0.00031205277009420124
loss: 0.0003057433106089318
loss: 0.0002976518319557611
loss: 0.00028667069111256224
loss: 0.0002762582304213822
loss: 0.00026832849137380224
test loss: 0.0002507518007572001
Save model!

EPOCH:  9
loss: 0.0002639818967853353
loss: 0.0002617535500163216
loss: 0.0002605056105544263
loss: 0.00025862725556417403
loss: 0.000255730941630928
loss: 0.0002554556958551075
loss: 0.0002522669597251027
loss: 0.00025123257994620407
loss: 0.0002500317378145148
loss: 0.00024819004798979353
loss: 0.00024574408286543456
loss: 0.00024233835769881657
loss: 0.00023732905301955594
loss: 0.00023244148967910895
loss: 0.00022770532837948173
loss: 0.00022521775426761928
loss: 0.00022329541487262015
loss: 0.00022189029362052864
loss: 0.00022022185815273005
loss: 0.00021814745735598427
test loss: 0.00021322915602768388
Save model!

EPOCH:  10
loss: 0.00021548389638531362
loss: 0.00021284222880334463
loss: 0.00021034642806733284
loss: 0.00020807592361003443
loss: 0.00020551241790882024
Traceback (most recent call last):
  File "ball_model.py", line 114, in <module>
    optimizer.step(closure)
  File "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lbfgs.py", line 213, in step
    loss = float(closure())
  File "ball_model.py", line 109, in closure
    out = seq(input)
  File "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "ball_model.py", line 48, in forward
    h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))
  File "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 684, in forward
    self.bias_ih, self.bias_hh,
  File "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/_functions/rnn.py", line 33, in LSTMCell
    gates = F.linear(input, w_ih, b_ih) + F.linear(hx, w_hh, b_hh)
  File "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py", line 992, in linear
    return torch.addmm(bias, input, weight.t())
KeyboardInterrupt
